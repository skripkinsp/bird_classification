import os
import re
import shutil
import subprocess

import numpy as np
import onnxruntime as ort
import torch
from dvc.repo import Repo
from kaggle.api.kaggle_api_extended import KaggleApi
from tqdm import tqdm


def merge_directories(src_dir, dst_dir):
    for item in os.listdir(src_dir):
        src_item = os.path.join(src_dir, item)
        dst_item = os.path.join(dst_dir, item)
        if os.path.isdir(src_item):
            if os.path.exists(dst_item):
                merge_directories(src_item, dst_item)
            else:
                shutil.move(src_item, dst_dir)
        else:
            shutil.move(src_item, dst_item)


def flatten_directory(base_dir):
    birds_dir = os.path.join(base_dir, "Birds")
    if os.path.exists(birds_dir) and os.path.isdir(birds_dir):
        for item in os.listdir(birds_dir):
            item_path = os.path.join(birds_dir, item)
            dst_path = os.path.join(base_dir, item)
            if os.path.exists(dst_path):
                print(f"Merging {item_path} into {dst_path}.")
                merge_directories(item_path, dst_path)
                os.rmdir(item_path)
            else:
                print(f"Moving {item_path} to {dst_path}.")
                shutil.move(item_path, base_dir)
        os.rmdir(birds_dir)
        print(f"Flattened directory structure in {base_dir}.")
    else:
        print(f"No 'Mushrooms' directory found in {base_dir}.")


def split_species_images(species_dir, output_dir, train_ratio, val_ratio, test_ratio):
    assert np.allclose(
        train_ratio + val_ratio + test_ratio, 1.0
    ), "Ratios must sum to 1.0"

    valid_extensions = (".png", ".jpg", ".jpeg")
    all_files = [
        f for f in os.listdir(species_dir) if f.lower().endswith(valid_extensions)
    ]

    if not all_files:
        print(f"No valid image files in {species_dir}. Skipping.")
        return

    files_with_numbers = []
    files_without_numbers = []
    for f in all_files:
        numbers = re.findall(r"\d+", f)
        if numbers:
            number = int(numbers[-1])
            files_with_numbers.append((f, number))
        else:
            files_without_numbers.append(f)

    files_with_numbers.sort(key=lambda x: x[1])
    sorted_with_numbers = [f for f, _ in files_with_numbers]

    files_without_numbers.sort()

    sorted_files = sorted_with_numbers + files_without_numbers

    num_files = len(sorted_files)
    train_end = int(train_ratio * num_files)
    val_end = train_end + int(val_ratio * num_files)

    class_name = os.path.basename(os.path.dirname(species_dir))
    species_name = os.path.basename(species_dir)

    for i, file in enumerate(sorted_files):
        if i < train_end:
            split_name = "train"
        elif i < val_end:
            split_name = "val"
        else:
            split_name = "test"

        split_species_dir = os.path.join(output_dir, split_name, class_name, species_name)
        os.makedirs(split_species_dir, exist_ok=True)
        shutil.copy(
            os.path.join(species_dir, file), os.path.join(split_species_dir, file)
        )


def split_data(data_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    os.makedirs(output_dir, exist_ok=True)

    for class_name in os.listdir(data_dir):
        class_path = os.path.join(data_dir, class_name)
        if class_path == output_dir:
            continue
        if os.path.isdir(class_path):
            for species_name in tqdm(os.listdir(class_path)):
                species_path = os.path.join(class_path, species_name)
                if os.path.isdir(species_path):
                    split_species_images(
                        species_path, output_dir, train_ratio, val_ratio, test_ratio
                    )


def prepare_data(data_dir, output_dir):
    """
    Prepare birds400 dataset by redistributing images.
    Move 15 images from each class in train to val, and 15 images to test.
    """
    import random
    random.seed(42)  # For reproducibility

    print("üîÑ Preparing birds400 dataset...")

    # Check if output_dir already exists and has data
    if os.path.exists(output_dir) and os.listdir(output_dir):
        print(f"‚úÖ Dataset already prepared at {output_dir}")
        return

    # Create output directories
    for split in ['train', 'val', 'test']:
        os.makedirs(os.path.join(output_dir, split), exist_ok=True)

    # Get all species (classes)
    species_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
    species_dirs.sort()

    print(f"üìä Found {len(species_dirs)} bird species")

    for species in species_dirs:
        species_path = os.path.join(data_dir, species)
        images = [f for f in os.listdir(species_path)
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        if len(images) < 45:  # Need at least 45 images per species (30 train + 15 val + 15 test)
            print(f"‚ö†Ô∏è  Species {species} has only {len(images)} images, skipping")
            continue

        # Shuffle images for random selection
        random.shuffle(images)

        # Split: first 15 for val, next 15 for test, rest for train
        val_images = images[:15]
        test_images = images[15:30]
        train_images = images[30:]

        print(f"  {species}: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test")

        # Create species directories in output
        for split in ['train', 'val', 'test']:
            os.makedirs(os.path.join(output_dir, split, species), exist_ok=True)

        # Move images to respective directories
        for img in val_images:
            src = os.path.join(species_path, img)
            dst = os.path.join(output_dir, 'val', species, img)
            shutil.copy2(src, dst)

        for img in test_images:
            src = os.path.join(species_path, img)
            dst = os.path.join(output_dir, 'test', species, img)
            shutil.copy2(src, dst)

        for img in train_images:
            src = os.path.join(species_path, img)
            dst = os.path.join(output_dir, 'train', species, img)
            shutil.copy2(src, dst)

    print(f"‚úÖ Dataset prepared at {output_dir}")
    print("üìà Distribution: ~30 images per class for training, 15 for validation, 15 for testing")


def get_git_commit_id():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode().strip()
    except Exception:
        return "unknown"


def download_data(data_dir: str, split_dir: str) -> None:
    # split_dir —Ç–µ–ø–µ—Ä—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ data_dir (birds400), –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å
    if os.path.exists(split_dir) and os.listdir(split_dir):
        print(f"Dataset found at {split_dir}")
        return

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ DVC
    try:
        repo = Repo(".")
        repo.pull(targets=[data_dir])
        print(f"Data successfully pulled from DVC to {data_dir}")
        return
    except Exception as e:
        print(f"DVC pull failed: {e}")

    # –ï—Å–ª–∏ DVC –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º —á–µ—Ä–µ–∑ Kaggle
    api = KaggleApi()
    dataset = "antoniozarauzmoreno/birds400"
    os.makedirs(data_dir, exist_ok=True)

    try:
        api.authenticate()
        print("Kaggle authentication successful!")
    except Exception as e:
        print(f"Kaggle authentication failed: {str(e)}")
        print("Please make sure you have:")
        print("1. Installed kaggle package: pip install kaggle")
        print("2. Downloaded your Kaggle API token from https://www.kaggle.com/account")
        print("3. Placed kaggle.json in ~/.kaggle/kaggle.json or C:\\Users\\<username>\\.kaggle\\kaggle.json")
        print("Cannot proceed without data!")
        exit(1)

    print(f"Downloading dataset {dataset} from Kaggle...")
    api.dataset_download_files(dataset, path=data_dir, unzip=True, quiet=False)

    print("Dataset downloaded successfully!")


def export_to_onnx(
    model,
    output_path: str,
    input_size: int = 224,
    device: str = "cuda",
):
    model.eval().to(device)

    dummy_input = torch.randn(1, 3, input_size, input_size).to(device)

    torch.onnx.export(
        model.model,
        dummy_input.cpu() if device == "cpu" else dummy_input,
        output_path,
        export_params=True,
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={
            "input": {0: "batch_size"},
            "output": {0: "batch_size"},
        },
        opset_version=13,
        do_constant_folding=True,
    )
    print(f"ONNX saved to {output_path}")

    ort_session = ort.InferenceSession(output_path)
    onnx_input = {
        ort_session.get_inputs()[0].name: (
            dummy_input.cpu().numpy() if device == "cuda" else dummy_input.numpy()
        )
    }
    _ = ort_session.run(None, onnx_input)[0]

    print("ONNX validated")
